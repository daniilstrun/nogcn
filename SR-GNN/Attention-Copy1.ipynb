{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ae3f2b-053f-4bd8-bea9-a8acd9e7f1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from itertools import repeat\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e89db1-ab7f-433b-b41b-639f20989a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 12, 12, 13])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.rand(11, 1, 12, 13)\n",
    "t2 = torch.rand(11, 12, 1, 13)\n",
    "(t1 + t2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c8b113-6c51-44af-bc63-f5cb7f11371c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./SR-GNN-master/pytorch_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6304b30a-3e9b-4df7-a0d6-2ac7b5547b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "989606b9-6940-4e0f-b4c1-5b706e19ff58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Opt():\n",
    "    def __init__(self, ):\n",
    "        self.dataset = 'yoochoose1_64'\n",
    "        self.batchSize = 100\n",
    "        self.hiddenSize = 100\n",
    "        self.epoch = 30\n",
    "        self.lr = 0.001\n",
    "        self.lr_dc = 0.1\n",
    "        self.lr_dc_step = 3\n",
    "        self.l2 = 1e-5\n",
    "        self.step = 1\n",
    "        self.patience = 10\n",
    "        self.nonhybrid = False\n",
    "        self.validation = True\n",
    "        self.valid_portion = 0.01\n",
    "        \n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11b20e6-416c-4b18-bbd8-04d695d2d05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./SR-GNN-master/datasets/' + opt.dataset + '/train.txt', 'rb'))\n",
    "if opt.validation:\n",
    "    train_data, valid_data = split_validation(train_data, opt.valid_portion)\n",
    "    test_data = valid_data\n",
    "else:\n",
    "    test_data = pickle.load(open('./SR-GNN-master/datasets/' + opt.dataset + '/test.txt', 'rb'))\n",
    "\n",
    "if opt.dataset == 'diginetica':\n",
    "    n_node = 43098\n",
    "elif opt.dataset == 'yoochoose1_64' or opt.dataset == 'yoochoose1_4':\n",
    "    n_node = 37484\n",
    "else:\n",
    "    n_node = 310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72830e85-d871-4d38-927d-c8492a9c12e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_ = set()\n",
    "for i in train_data[0]:\n",
    "    for j in i:\n",
    "        set_.add(j)\n",
    "for i in train_data[1]:\n",
    "    set_.add(i)\n",
    "item_num = len(set_)\n",
    "\n",
    "item_mapping = {}\n",
    "for i, item in enumerate(set_):\n",
    "    item_mapping[item] = i\n",
    "\n",
    "d_basket_train = {k: [item_mapping[item] for item in items + [train_data[1][k]]] for k, items in enumerate(train_data[0])}\n",
    "d_basket_test = {k: [item_mapping[item] for item in items + [test_data[1][k]] if item in item_mapping] for k, items in enumerate(test_data[0])}\n",
    "d_basket_test = {k: v for k, v in d_basket_test.items() if len(v) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a8439e-3f0f-473e-84d0-09faf9e47052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoaderStoch():\n",
    "    '''\n",
    "    Класс автоматическрого создания батчей для трейна.\n",
    "    '''\n",
    "    def __init__(self, trans,\n",
    "                 basket,\n",
    "                 batchsize=128,\n",
    "                 max_basket_size=23,\n",
    "                 item_num=40,\n",
    "                 shuffle=True):\n",
    "        ''''\n",
    "        На вход:\n",
    "        trans          - список id чеков\n",
    "        basket         - словарь товаров в коризне по чеку,\n",
    "                         в виде списка из номеров товаров\n",
    "        item_num  - кол-во товаров\n",
    "        batchsize - размер батча\n",
    "        shuffle   - перемешивать ли семплы\n",
    "        На выход:\n",
    "        батч в виде [(\n",
    "                      корзины: list[list],\n",
    "                      контекст: Tensor, size=(batchsize, context_dim),\n",
    "                      таргетные продукты: LongTensor, size=(batchsize,),\n",
    "                      клиенты: LongTensor, size=(batchsize,)\n",
    "                     ),\n",
    "                     таргет: Tensor, size=(batchsize,)]\n",
    "        '''\n",
    "        self.trans = trans\n",
    "        self.basket = basket\n",
    "        self.max_basket_size = max_basket_size\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "        self.prod_num = item_num\n",
    "        \n",
    "    def __iter__(self):  \n",
    "        '''\n",
    "        Метод вызывается при итерировании по объекту,\n",
    "        например, через for.\n",
    "        '''\n",
    "        self.ids = set(range(len(self.trans)))\n",
    "        return self._contaner_()\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Возвращает кол-во батчей.\n",
    "        '''\n",
    "        batch_num = np.ceil(len(self.trans)/self.batchsize)\n",
    "        return int(batch_num)\n",
    "        \n",
    "    def _contaner_(self):\n",
    "        '''\n",
    "        Метод берет подвыборку и формирует батч.\n",
    "        '''\n",
    "        while len(self.ids) != 0:\n",
    "            if self.shuffle:\n",
    "                size = min(len(self.ids), self.batchsize)\n",
    "                idx_curr = np.random.choice(list(self.ids), size,\n",
    "                                            replace=False)\n",
    "            else:\n",
    "                idx_curr = np.array(list(self.ids))[:self.batchsize]\n",
    "            self.ids = self.ids.difference(idx_curr)\n",
    "            yield self._make_sample_(self.trans[idx_curr])\n",
    "        \n",
    "    def foo(self, trans):\n",
    "        '''\n",
    "        Вспомогательная функция. см _make_sample_.\n",
    "        '''\n",
    "        trans_products = self.basket[trans]\n",
    "        target_prod = trans_products[-1]\n",
    "        without_target = trans_products[:-1]\n",
    "        padding = [self.prod_num]*(self.max_basket_size - len(without_target))\n",
    "        \n",
    "        return padding+without_target, target_prod\n",
    "        \n",
    "    def _make_sample_(self, X):\n",
    "        '''\n",
    "        Метод возвращает готовый батч.\n",
    "        '''\n",
    "        self.X = X\n",
    "        temp = list(map(self.foo, self.X))\n",
    "\n",
    "        return [torch.LongTensor(np.array([row[0] for row in temp])),\n",
    "                torch.LongTensor(np.array([row[1] for row in temp]))]\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8dd029-b32c-4563-89ea-59819ec039be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#функция тестирования\n",
    "def test(model, test_data):\n",
    "    model.eval()\n",
    "    hit, mrr = [], []\n",
    "    batch_size = min(opt.batchSize, len(test_data[0]))\n",
    "    for i in tqdm(range(0, len(test_data[0]), batch_size)):\n",
    "        scores = model.predict(test_data[0][i:i+batch_size])\n",
    "        sub_scores = np.array(scores.topk(20)[1])\n",
    "        targets = test_data[1][i:i+batch_size]\n",
    "        for score, target in zip(sub_scores, targets):\n",
    "            target = model.item_mapping.get(target)\n",
    "            hit.append(np.isin(target, score))\n",
    "            if len(np.where(score == target)[0]) == 0:\n",
    "                mrr.append(0)\n",
    "            else:\n",
    "                mrr.append(1 / (np.where(score == target)[0][0] + 1))\n",
    "    hit = np.mean(hit) * 100\n",
    "    mrr = np.mean(mrr) * 100\n",
    "    return hit, mrr\n",
    "\n",
    "#функция тестирования\n",
    "def evaluate_net(net, testloader, use_cuda=True):\n",
    "    net = net.eval()\n",
    "    running_loss = 0.0\n",
    "    # цикл по батчам внутри эпохи\n",
    "    for i, data in enumerate(tqdm(testloader)):\n",
    "        # берем очередной батч и его лейблы\n",
    "        prods = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "        \n",
    "        # получили выход сетки\n",
    "        outputs = net(prods)\n",
    "        \n",
    "        # посчитали для этого выхода лосс\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #суммируемый лосс на обучении\n",
    "        running_loss += float(loss)\n",
    "        \n",
    "    return running_loss/len(testloader)\n",
    "\n",
    "#основная функция для обучения сети\n",
    "def train_net(n_epochs, \n",
    "              net, \n",
    "              optimizer, \n",
    "              scheduler,\n",
    "              criterion, \n",
    "              trainloader,\n",
    "              testloader,\n",
    "              test_data,\n",
    "              prod_num,\n",
    "              use_cuda=False,\n",
    "             ):\n",
    "    '''\n",
    "    Функция обучения нейронной сети.\n",
    "    На вход:\n",
    "    n_epochs      - кол-во эпох\n",
    "    net           - сеть для обучения\n",
    "    optimizer     - оптимизатор для обучения\n",
    "    criterion     - критерий оптимизации\n",
    "    trainloader   - даталоадер для трейна\n",
    "    testloader    - даталоадер для теста\n",
    "    d_food_cost_idx - цены на товары\n",
    "    prod_num      - кол-во продуктов\n",
    "    use_cuda      - использовать ли cuda\n",
    "    verbose       - если 0, то не выводит \n",
    "                    качество на валидации,\n",
    "                    если > 0 выводит качество\n",
    "                    на валлидации каждые verbose\n",
    "                    эпох\n",
    "    early_stopping_len - после какого кол-ва эпох\n",
    "                    без улучшения качества \n",
    "                    надо прекратить обучение\n",
    "    '''\n",
    "    \n",
    "    if use_cuda:\n",
    "        net = net.cuda()\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    # основной цикл по всем эпохам\n",
    "    for epoch in range(n_epochs):\n",
    "        net = net.train()\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        # цикл по батчам внутри эпохи\n",
    "        for i, data in enumerate(tqdm(trainloader)):\n",
    "            # берем очередной батч и его лейблы\n",
    "            prods = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            # всегда перед вычислением градиентов зануляем их, чтобы не накапливались\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # получили выход сетки\n",
    "            outputs = net(prods)\n",
    "            \n",
    "            # посчитали для этого выхода лосс\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # вычислили градиенты loss по параметрам сети (w)\n",
    "            loss.backward()\n",
    "    \n",
    "            #далем шаг по антиградиенту - обновляем веса сети\n",
    "            optimizer.step()\n",
    "            \n",
    "            #суммируемый лосс на обучении\n",
    "            running_loss += float(loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        # валидируемся\n",
    "        hit, mrr = test(net, test_data)\n",
    "        test_loss = evaluate_net(net, testloader, use_cuda=use_cuda)\n",
    "        \n",
    "        # логируем после каждой эпохи        \n",
    "        print('Epoch {}. \\nTrain_loss: {:.6f}' .format(epoch + 1, running_loss / len(trainloader)))\n",
    "        print(f'Test_loss: {test_loss}')\n",
    "        print(f'Test eval: hit - {hit}, mrr - {mrr}')\n",
    "        print('------------------------------')\n",
    "\n",
    "    print('Finished Training')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3243b9c5-7c86-4af7-a643-b8ccd998ae96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "class Embedding_prod(nn.Module):\n",
    "    '''\n",
    "    Нейросеть для эмбеддинга товара.\n",
    "    '''\n",
    "    def __init__(self, prod_num, d, dropout=0.1):\n",
    "        super(Embedding_prod, self).__init__()\n",
    "        self.prod_embedd = nn.Embedding(prod_num+1, d)\n",
    "        self.pos_encoder = PositionalEncoding(d, dropout)\n",
    "        \n",
    "    def forward(self, prods):\n",
    "        prod_embedd = self.prod_embedd(prods)\n",
    "        prod_embedd = self.pos_encoder(prod_embedd)\n",
    "        return prod_embedd\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x).permute(1, 0, 2)\n",
    "\n",
    "class Embedding_transformer(nn.Module):\n",
    "    '''\n",
    "    Нейросеть для эмбеддинга товара.\n",
    "    '''\n",
    "    def __init__(self, d, layer_num=2, layer_type='transformer', dim_feedforward=2048, dropout=0.1, max_basket_size=146):\n",
    "        '''\n",
    "        На вход:\n",
    "        X - матрица (товары, эмбеддинг)\n",
    "        use_cuda - использовать ли cuda\n",
    "        \n",
    "        Forward принимает на вход или\n",
    "        список, содержащий номера товаров,\n",
    "        или просто номер товара.\n",
    "        '''\n",
    "        super(Embedding_transformer, self).__init__()\n",
    "        self.layer_type = layer_type\n",
    "        if layer_type == 'attention':\n",
    "            self.transformer = nn.ModuleList([nn.MultiheadAttention(d, \n",
    "                                                                    num_heads=1,\n",
    "                                                                    batch_first=True,\n",
    "                                                                    )\n",
    "                                          for _ in range(layer_num)])\n",
    "        elif layer_type == 'transformer':\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=d,\n",
    "                                                       nhead=1,\n",
    "                                                       dropout=dropout,\n",
    "                                                       dim_feedforward=dim_feedforward,\n",
    "                                                       batch_first=True,\n",
    "                                                      )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=layer_num)\n",
    "        #self.transformer_layer = nn.TransformerEncoderLayer(d,\n",
    "        #                                                   nhead=1,\n",
    "        #                                                   batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #t = self.transformer(batch)\n",
    "        t = batch\n",
    "        if self.layer_type == 'attention':\n",
    "            for layer in self.transformer:\n",
    "                t_ = t\n",
    "                t = layer(t, t, t)[0]\n",
    "                t = t_ + t\n",
    "        elif self.layer_type == 'transformer':\n",
    "            t = self.transformer(t)\n",
    "        return t\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, products_embedd, transformer,\n",
    "                 d, max_basket_size, prod_num, item_mapping=item_mapping):\n",
    "        '''\n",
    "        На вход:\n",
    "        products_embedd - объект класса Embedding_prod\n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = products_embedd\n",
    "        self.transformer = transformer\n",
    "        self.max_basket_size = max_basket_size\n",
    "        self.linear1 = nn.Linear(max_basket_size, 1)\n",
    "        self.linear2 = nn.Linear(d, prod_num)\n",
    "        self.max_basket_size = max_basket_size\n",
    "        self.prod_num = prod_num\n",
    "        self.item_mapping = item_mapping\n",
    "    \n",
    "    def predict(self, baskets):\n",
    "        rows = []\n",
    "        for basket in baskets:\n",
    "            basket_ = [self.item_mapping[item] for item in basket if item in item_mapping]\n",
    "            padding = [self.prod_num]*(self.max_basket_size - len(basket_))\n",
    "            rows.append(padding + basket_)\n",
    "        return self.forward(torch.LongTensor(np.array(rows)))\n",
    "    \n",
    "    def forward(self, prods):\n",
    "        # эмделлинг товара [batch_dim, max_basket_size, embedd_dim]\n",
    "        embedd = self.embedding(prods)\n",
    "        # пропускаем через череду attention\n",
    "        embedd = self.transformer(embedd)\n",
    "        # пропускаем чеоез линейныt слои\n",
    "        embedd = embedd.permute(0, 2, 1)\n",
    "        embedd = self.linear1(embedd).squeeze(2)\n",
    "        return self.linear2(embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f553bfd-3538-4c0d-9fca-d90d84976fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_basket_size = max(map(len, train_data[0]))+1\n",
    "max_basket_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "542c0f7f-27c1-4dda-ae77-84a6f1c1781c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0916050-fa07-4e8e-811c-e83e073ace3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding_prod(\n",
       "    (prod_embedd): Embedding(17353, 32)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (transformer): Embedding_transformer(\n",
       "    (transformer): ModuleList(\n",
       "      (0): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=146, out_features=1, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=17352, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создаем сеть\n",
    "embedd_dim = 32 # 512\n",
    "prod_embedd = Embedding_prod(item_num, embedd_dim)\n",
    "transformer = Embedding_transformer(embedd_dim,\n",
    "                                    layer_num=1,\n",
    "                                    layer_type='attention',\n",
    "                                    dim_feedforward=64,\n",
    "                                    dropout=0.1,\n",
    "                                    max_basket_size=max_basket_size)\n",
    "net_model = Net(prod_embedd, transformer, embedd_dim, max_basket_size, item_num)\n",
    "net_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8b2a84d-4301-4f72-bc09-5387eb499ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "learning_rate= 1e-3 # 1e-4\n",
    "optimizer = optim.Adam(net_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8, verbose=True) # 0.8\n",
    "\n",
    "# критерий оптимизации\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74838e51-f085-4e6a-bf87-af33fc063804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_num_threads(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "967c97f2-0b95-407e-8ec1-fd385a0b7b16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfd8a90a34e4cc886b9e67a2bd4b375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 8.0000e-04.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3c7b7c9d724863b6952f6504328f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43211c67415c45c08ea6bc7efd5f03d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. \n",
      "Train_loss: 6.983041\n",
      "Test_loss: 6.011591335822796\n",
      "Test eval: hit - 10.948905109489052, mrr - 3.1867496733764664\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c96953a505c436b86ecc01523303e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 6.4000e-04.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786dfbd9bb834caf87450eed8793fb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1a88cbb52149f584074ec8e8cc742a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2. \n",
      "Train_loss: 5.841795\n",
      "Test_loss: 5.505891651942812\n",
      "Test eval: hit - 12.192484455258178, mrr - 3.9733400386643636\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa849e879dd4098bf573f1fcd18efe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.1200e-04.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3d9353c10847b0bcd9e45bad4c546c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca54629102a46fba2aea17ceec5844c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3. \n",
      "Train_loss: 5.393295\n",
      "Test_loss: 5.279505713232632\n",
      "Test eval: hit - 14.165990808326576, mrr - 4.472298035233838\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c829a999b98141d98fa10c97c81754dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m dataloader_test \u001b[38;5;241m=\u001b[39m DataLoaderStoch(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(d_basket_test\u001b[38;5;241m.\u001b[39mkeys())),\n\u001b[1;32m     10\u001b[0m                       d_basket_test,\n\u001b[1;32m     11\u001b[0m                       max_basket_size\u001b[38;5;241m=\u001b[39mmax_basket_size,\n\u001b[1;32m     12\u001b[0m                       batchsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     13\u001b[0m                       shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# учим сеть\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m net_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnet_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mitem_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                   \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 102\u001b[0m, in \u001b[0;36mtrain_net\u001b[0;34m(n_epochs, net, optimizer, scheduler, criterion, trainloader, testloader, test_data, prod_num, use_cuda)\u001b[0m\n\u001b[1;32m     99\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# вычислили градиенты loss по параметрам сети (w)\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m#далем шаг по антиградиенту - обновляем веса сети\u001b[39;00m\n\u001b[1;32m    105\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# создаем даталоадер для обучения\n",
    "dataloader_train = DataLoaderStoch(np.array(list(d_basket_train.keys())),\n",
    "                      d_basket_train,\n",
    "                      max_basket_size=max_basket_size,\n",
    "                      batchsize=64,\n",
    "                      shuffle=True)\n",
    "\n",
    "# создаем даталоадер для валидации\n",
    "dataloader_test = DataLoaderStoch(np.array(list(d_basket_test.keys())),\n",
    "                      d_basket_test,\n",
    "                      max_basket_size=max_basket_size,\n",
    "                      batchsize=64,\n",
    "                      shuffle=True)\n",
    "\n",
    "# учим сеть\n",
    "net_model = train_net(50,\n",
    "                   net_model,\n",
    "                   optimizer,\n",
    "                   scheduler,\n",
    "                   criterion,\n",
    "                   dataloader_train,\n",
    "                   dataloader_test,\n",
    "                   test_data,\n",
    "                   item_num,\n",
    "                   use_cuda=False,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc771681-7448-411f-addc-d74784586bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530aa945-be55-4a5f-80f1-29ba1142bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ca39d26-faf2-42af-8bcb-7c8448478089",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3863)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "C = 4\n",
    "d = 20\n",
    "\n",
    "target = torch.LongTensor(torch.randint(C, (N, d)))\n",
    "scores = torch.zeros(N, C, d)\n",
    "criterion(scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a05fd2a-d65e-481d-a3b4-ee466dfdd4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 2, 1, 3, 1, 3, 1, 1, 1, 2, 0, 1, 3, 0, 0, 1, 1, 1],\n",
       "        [3, 3, 3, 0, 3, 3, 1, 1, 2, 3, 3, 0, 0, 3, 1, 1, 3, 3, 0, 1],\n",
       "        [0, 2, 1, 2, 0, 1, 1, 3, 2, 2, 1, 0, 0, 3, 1, 1, 0, 0, 0, 2],\n",
       "        [0, 0, 1, 0, 3, 2, 0, 1, 1, 3, 2, 3, 1, 0, 0, 2, 3, 1, 0, 0],\n",
       "        [1, 2, 0, 2, 1, 1, 0, 2, 2, 3, 2, 0, 0, 2, 0, 1, 1, 0, 1, 2],\n",
       "        [3, 0, 3, 1, 1, 0, 2, 0, 3, 1, 1, 3, 2, 1, 3, 0, 1, 3, 1, 0],\n",
       "        [2, 1, 3, 3, 2, 1, 1, 1, 2, 0, 2, 1, 2, 3, 1, 2, 0, 0, 0, 1],\n",
       "        [1, 3, 3, 1, 3, 0, 0, 0, 2, 0, 1, 3, 1, 1, 2, 3, 0, 1, 3, 3],\n",
       "        [2, 0, 1, 3, 1, 0, 2, 0, 1, 2, 0, 2, 0, 0, 1, 0, 2, 1, 0, 0],\n",
       "        [1, 3, 0, 3, 0, 1, 2, 0, 3, 1, 0, 0, 0, 2, 0, 2, 2, 1, 1, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e834fb6-5a8a-400c-9abb-4292d174aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0163ef5-6c81-49d8-8280-06ccda689dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer = torch.nn.Transformer(batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde01680-d249-49c4-8ff5-93240782b84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 20, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 12\n",
    "C = 4\n",
    "Lin = 10\n",
    "Lout = 20\n",
    "d = 512\n",
    "input_ = torch.rand((N, Lin, d))\n",
    "output_ = torch.rand((N, Lout, d))\n",
    "transformer(input_, output_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "234554c8-1d54-4ab8-b412-f8644df1325d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a464036-3569-4bd0-b881-561dc357f8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
