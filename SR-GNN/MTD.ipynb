{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ae3f2b-053f-4bd8-bea9-a8acd9e7f1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from itertools import repeat\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c8b113-6c51-44af-bc63-f5cb7f11371c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./SR-GNN-master/pytorch_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6304b30a-3e9b-4df7-a0d6-2ac7b5547b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "989606b9-6940-4e0f-b4c1-5b706e19ff58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Opt():\n",
    "    def __init__(self, ):\n",
    "        self.dataset = 'diginetica'\n",
    "        self.batchSize = 100\n",
    "        self.hiddenSize = 100\n",
    "        self.epoch = 30\n",
    "        self.lr = 0.001\n",
    "        self.lr_dc = 0.1\n",
    "        self.lr_dc_step = 3\n",
    "        self.l2 = 1e-5\n",
    "        self.step = 1\n",
    "        self.patience = 10\n",
    "        self.nonhybrid = False\n",
    "        self.validation = False\n",
    "        self.valid_portion = 0.1\n",
    "        \n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11b20e6-416c-4b18-bbd8-04d695d2d05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./SR-GNN-master/datasets/' + opt.dataset + '/train.txt', 'rb'))\n",
    "if opt.validation:\n",
    "    train_data, valid_data = split_validation(train_data, opt.valid_portion)\n",
    "    test_data = valid_data\n",
    "else:\n",
    "    test_data = pickle.load(open('./SR-GNN-master/datasets/' + opt.dataset + '/test.txt', 'rb'))\n",
    "\n",
    "if opt.dataset == 'diginetica':\n",
    "    n_node = 43098\n",
    "elif opt.dataset == 'yoochoose1_64' or opt.dataset == 'yoochoose1_4':\n",
    "    n_node = 37484\n",
    "else:\n",
    "    n_node = 310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29a04f6c-f8eb-4478-8dc6-335cc8b22314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_seqs = pickle.load(open('./SR-GNN-master/datasets/' + opt.dataset + '/all_train_seq.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72830e85-d871-4d38-927d-c8492a9c12e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_ = set()\n",
    "for i in train_data[0]:\n",
    "    for j in i:\n",
    "        set_.add(j)\n",
    "for i in train_data[1]:\n",
    "    set_.add(i)\n",
    "item_num = len(set_)\n",
    "\n",
    "item_mapping = {}\n",
    "for i, item in enumerate(set_):\n",
    "    item_mapping[item] = i\n",
    "\n",
    "n = int((1-opt.valid_portion) * len(train_seqs))\n",
    "d_basket_train = {k: [item_mapping[item] for item in items] for k, items in enumerate(train_seqs[:-1])}\n",
    "d_basket_val = {k: [item_mapping[item] for item in items] for k, items in enumerate(train_seqs[1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07a3e012-176c-4ea7-8fcb-47ac4e2a2930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_p = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5a8439e-3f0f-473e-84d0-09faf9e47052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoaderStoch():\n",
    "    '''\n",
    "    Класс автоматическрого создания батчей для трейна.\n",
    "    '''\n",
    "    def __init__(self, trans,\n",
    "                 basket,\n",
    "                 item_num,\n",
    "                 batchsize=128,\n",
    "                 max_basket_size=23,\n",
    "                 shuffle=True):\n",
    "        ''''\n",
    "        На вход:\n",
    "        trans          - список id чеков\n",
    "        basket         - словарь товаров в коризне по чеку,\n",
    "                         в виде списка из номеров товаров\n",
    "        item_num  - кол-во товаров\n",
    "        batchsize - размер батча\n",
    "        shuffle   - перемешивать ли семплы\n",
    "        На выход:\n",
    "        батч в виде [(\n",
    "                      корзины: list[list],\n",
    "                      контекст: Tensor, size=(batchsize, context_dim),\n",
    "                      таргетные продукты: LongTensor, size=(batchsize,),\n",
    "                      клиенты: LongTensor, size=(batchsize,)\n",
    "                     ),\n",
    "                     таргет: Tensor, size=(batchsize,)]\n",
    "        '''\n",
    "        self.trans = trans\n",
    "        self.basket = basket\n",
    "        self.max_basket_size = max_basket_size\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "        self.prod_num = item_num\n",
    "        \n",
    "    def __iter__(self):  \n",
    "        '''\n",
    "        Метод вызывается при итерировании по объекту,\n",
    "        например, через for.\n",
    "        '''\n",
    "        self.ids = set(range(len(self.trans)))\n",
    "        return self._contaner_()\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Возвращает кол-во батчей.\n",
    "        '''\n",
    "        batch_num = np.ceil(len(self.trans)/self.batchsize)\n",
    "        return int(batch_num)\n",
    "        \n",
    "    def _contaner_(self):\n",
    "        '''\n",
    "        Метод берет подвыборку и формирует батч.\n",
    "        '''\n",
    "        while len(self.ids) != 0:\n",
    "            if self.shuffle:\n",
    "                size = min(len(self.ids), self.batchsize)\n",
    "                idx_curr = np.random.choice(list(self.ids), size,\n",
    "                                            replace=False)\n",
    "            else:\n",
    "                idx_curr = np.array(list(self.ids))[:self.batchsize]\n",
    "            self.ids = self.ids.difference(idx_curr)\n",
    "            yield self._make_sample_(self.trans[idx_curr])\n",
    "        \n",
    "    def foo(self, trans):\n",
    "        '''\n",
    "        Вспомогательная функция. см _make_sample_.\n",
    "        '''\n",
    "        trans_products = self.basket[trans]\n",
    "        padding = [self.prod_num]*(self.max_basket_size - len(trans_products)+global_p)\n",
    "        return (padding+trans_products)[:-1], (padding+trans_products)[-1]\n",
    "        \n",
    "    def _make_sample_(self, X):\n",
    "        '''\n",
    "        Метод возвращает готовый батч.\n",
    "        '''\n",
    "        self.X = X\n",
    "        temp = list(map(self.foo, self.X))\n",
    "\n",
    "        return [torch.LongTensor(np.array([row[0] for row in temp])),\n",
    "                torch.LongTensor(np.array([row[1] for row in temp]))]\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad8dd029-b32c-4563-89ea-59819ec039be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#функция тестирования\n",
    "def test(model, test_data):\n",
    "    model.eval()\n",
    "    hit, mrr = [], []\n",
    "    batch_size = min(opt.batchSize, len(test_data[0]))\n",
    "    for i in tqdm(range(0, len(test_data[0]), batch_size)):\n",
    "        scores = model.predict(test_data[0][i:i+batch_size])\n",
    "        sub_scores = np.array(scores.topk(20)[1])\n",
    "        targets = test_data[1][i:i+batch_size]\n",
    "        for score, target in zip(sub_scores, targets):\n",
    "            target = model.item_mapping.get(target)\n",
    "            hit.append(np.isin(target, score))\n",
    "            if len(np.where(score == target)[0]) == 0:\n",
    "                mrr.append(0)\n",
    "            else:\n",
    "                mrr.append(1 / (np.where(score == target)[0][0] + 1))\n",
    "    hit = np.mean(hit) * 100\n",
    "    mrr = np.mean(mrr) * 100\n",
    "    return hit, mrr\n",
    "\n",
    "#функция тестирования\n",
    "def evaluate_net(net, testloader, use_cuda=True):\n",
    "    net = net.eval()\n",
    "    running_loss = 0.0\n",
    "    # цикл по батчам внутри эпохи\n",
    "    for i, data in enumerate(tqdm(testloader)):\n",
    "        # берем очередной батч и его лейблы\n",
    "        prods = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "        \n",
    "        # получили выход сетки\n",
    "        outputs = net(prods)\n",
    "        \n",
    "        # посчитали для этого выхода лосс\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #суммируемый лосс на обучении\n",
    "        running_loss += float(loss)\n",
    "        \n",
    "    return running_loss/len(testloader)\n",
    "\n",
    "#основная функция для обучения сети\n",
    "def train_net(n_epochs, \n",
    "              net, \n",
    "              optimizer, \n",
    "              scheduler,\n",
    "              criterion, \n",
    "              trainloader,\n",
    "              testloader,\n",
    "              test_data,\n",
    "              prod_num,\n",
    "              use_cuda=False,\n",
    "             ):\n",
    "    '''\n",
    "    Функция обучения нейронной сети.\n",
    "    На вход:\n",
    "    n_epochs      - кол-во эпох\n",
    "    net           - сеть для обучения\n",
    "    optimizer     - оптимизатор для обучения\n",
    "    criterion     - критерий оптимизации\n",
    "    trainloader   - даталоадер для трейна\n",
    "    testloader    - даталоадер для теста\n",
    "    d_food_cost_idx - цены на товары\n",
    "    prod_num      - кол-во продуктов\n",
    "    use_cuda      - использовать ли cuda\n",
    "    verbose       - если 0, то не выводит \n",
    "                    качество на валидации,\n",
    "                    если > 0 выводит качество\n",
    "                    на валлидации каждые verbose\n",
    "                    эпох\n",
    "    early_stopping_len - после какого кол-ва эпох\n",
    "                    без улучшения качества \n",
    "                    надо прекратить обучение\n",
    "    '''\n",
    "    \n",
    "    if use_cuda:\n",
    "        net = net.cuda()\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    # основной цикл по всем эпохам\n",
    "    for epoch in range(n_epochs):\n",
    "        net = net.train()\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        # цикл по батчам внутри эпохи\n",
    "        for i, data in enumerate(tqdm(trainloader)):\n",
    "            # берем очередной батч и его лейблы\n",
    "            prods = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            # всегда перед вычислением градиентов зануляем их, чтобы не накапливались\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # получили выход сетки\n",
    "            outputs = net(prods)\n",
    "\n",
    "            # посчитали для этого выхода лосс\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # вычислили градиенты loss по параметрам сети (w)\n",
    "            loss.backward()\n",
    "\n",
    "            #далем шаг по антиградиенту - обновляем веса сети\n",
    "            optimizer.step()\n",
    "\n",
    "            #суммируемый лосс на обучении\n",
    "            running_loss += float(loss)\n",
    "        # валидируемся\n",
    "        hit, mrr = test(net, test_data)\n",
    "        # test_loss = evaluate_net(net, testloader, use_cuda=use_cuda)\n",
    "        # torch.save(net.state_dict(), 'net.model')\n",
    "        # логируем после каждой эпохи        \n",
    "        print('Epoch {}. \\nTrain_loss: {:.6f}' .format(epoch + 1, running_loss / len(trainloader)))\n",
    "        # print(f'Test_loss: {test_loss}')\n",
    "        print(f'Test eval: hit - {hit}, mrr - {mrr}')\n",
    "        print('------------------------------')\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Finished Training')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3243b9c5-7c86-4af7-a643-b8ccd998ae96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "\n",
    "class Embedding_prod(nn.Module):\n",
    "    '''\n",
    "    Нейросеть для эмбеддинга товара.\n",
    "    '''\n",
    "    def __init__(self, data, item_num, item_mapping):\n",
    "        super(Embedding_prod, self).__init__()\n",
    "        self.item_mapping = item_mapping\n",
    "        Dii = defaultdict(int)\n",
    "        for items in tqdm(train_seqs, total=len(train_seqs)):\n",
    "            session_len = len(items)\n",
    "            for i in range(session_len-1):\n",
    "                Dii[self.item_mapping[items[i]], self.item_mapping[items[i+1]]] += 1\n",
    "        \n",
    "        i = torch.LongTensor(list(Dii.keys())).T\n",
    "        v = torch.LongTensor(list(Dii.values()))\n",
    "        Q = torch.sparse_coo_tensor(i, v, (item_num+1, item_num+1)).to_dense()\n",
    "        \n",
    "        prod_embedd = nn.Embedding(item_num+1, item_num+1).requires_grad_(False)\n",
    "        prod_embedd.weight = nn.Parameter(Q/(Q.sum(1)[:, None] + 1e-6))\n",
    "        #prod_embedd.requires_grad = False\n",
    "        self.prod_embedd = prod_embedd.requires_grad_(False)\n",
    "        \n",
    "    def forward(self, prods):\n",
    "        prod_embedd = self.prod_embedd(prods)\n",
    "        return prod_embedd\n",
    "\n",
    "\n",
    "class Embedding_transformer(nn.Module):\n",
    "    '''\n",
    "    Нейросеть для эмбеддинга товара.\n",
    "    '''\n",
    "    def __init__(self, p=3):\n",
    "        '''\n",
    "        На вход:\n",
    "        X - матрица (товары, эмбеддинг)\n",
    "        use_cuda - использовать ли cuda\n",
    "        \n",
    "        Forward принимает на вход или\n",
    "        список, содержащий номера товаров,\n",
    "        или просто номер товара.\n",
    "        '''\n",
    "        super(Embedding_transformer, self).__init__()\n",
    "        self.p = p\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.base = nn.Parameter(torch.tensor(2.))\n",
    "        self.params = torch.Tensor(list(range(p))[::-1])\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        t = batch[:, -self.p:, :] / (self.base**self.params[None, :, None])\n",
    "        return t.sum(1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, products_embedd, transformer, max_basket_size, item_mapping=item_mapping):\n",
    "        '''\n",
    "        На вход:\n",
    "        products_embedd - объект класса Embedding_prod\n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = products_embedd\n",
    "        self.transformer = transformer\n",
    "        self.max_basket_size = max_basket_size\n",
    "        self.item_mapping = item_mapping\n",
    "        self.prod_num = len(item_mapping)\n",
    "    \n",
    "    def predict(self, baskets):\n",
    "        rows = []\n",
    "        range_batch = range(len(baskets))\n",
    "        for basket in baskets:\n",
    "            basket_ = [self.item_mapping[item] for item in basket if item in item_mapping]\n",
    "            padding = [self.prod_num]*(self.max_basket_size - len(basket_) + global_p - 1)\n",
    "            rows.append(padding+basket_)\n",
    "        return self.forward(torch.LongTensor(np.array(rows)))\n",
    "    \n",
    "    def forward(self, prods):\n",
    "        embedd = self.embedding(prods)\n",
    "        return self.transformer(embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f553bfd-3538-4c0d-9fca-d90d84976fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_basket_size = max(map(len, train_seqs))\n",
    "max_basket_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "542c0f7f-27c1-4dda-ae77-84a6f1c1781c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0916050-fa07-4e8e-811c-e83e073ace3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1f40db8db74a09b02b1654519915b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding_prod(\n",
       "    (prod_embedd): Embedding(43098, 43098)\n",
       "  )\n",
       "  (transformer): Embedding_transformer(\n",
       "    (sigm): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создаем сеть\n",
    "embedd_dim = 1024 # 512\n",
    "prod_embedd = Embedding_prod(train_seqs, item_num, item_mapping)\n",
    "transformer = Embedding_transformer(p=global_p)\n",
    "net_model = Net(prod_embedd, transformer, max_basket_size, item_mapping)\n",
    "net_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17362c8a-3bed-4b33-932f-dcdb0b632d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_model.embedding.prod_embedd.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a1562ce-09eb-415e-a703-66958b35bb70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.prod_embedd.weight',\n",
       "              tensor([[0.0000, 0.1429, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "                      [0.5000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "                      [0.0000, 0.0000, 0.2055,  ..., 0.0000, 0.0000, 0.0000],\n",
       "                      ...,\n",
       "                      [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "                      [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "                      [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])),\n",
       "             ('transformer.base', tensor(2.))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fdf3dd7-b297-4fe8-b062-38f479913dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_model.load_state_dict(torch.load('net.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8b2a84d-4301-4f72-bc09-5387eb499ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "learning_rate= 1e-1 # 1e-4\n",
    "optimizer = optim.Adam(net_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8, verbose=True) # 0.8\n",
    "\n",
    "# критерий оптимизации\n",
    "weight = torch.ones(item_num+1)\n",
    "weight[-1] = 0\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74838e51-f085-4e6a-bf87-af33fc063804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_num_threads(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "967c97f2-0b95-407e-8ec1-fd385a0b7b16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7c673de0fe46b2af67b40ffd5d30f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2323b031e1a34b15969cac185262346b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. \n",
      "Train_loss: 10.580825\n",
      "Test eval: hit - 34.03825298235236, mrr - 12.945250365614204\n",
      "------------------------------\n",
      "Adjusting learning rate of group 0 to 8.0000e-02.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3245803f0f486bb53522a2becb5327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff57fdcc798495f87d30452b7d05d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m dataloader_val \u001b[38;5;241m=\u001b[39m DataLoaderStoch(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(d_basket_val\u001b[38;5;241m.\u001b[39mkeys())),\n\u001b[1;32m     11\u001b[0m                       d_basket_val,\n\u001b[1;32m     12\u001b[0m                       item_num\u001b[38;5;241m=\u001b[39mitem_num,\n\u001b[1;32m     13\u001b[0m                       max_basket_size\u001b[38;5;241m=\u001b[39mmax_basket_size,\n\u001b[1;32m     14\u001b[0m                       batchsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     15\u001b[0m                       shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# учим сеть\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m net_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnet_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mitem_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                   \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 110\u001b[0m, in \u001b[0;36mtrain_net\u001b[0;34m(n_epochs, net, optimizer, scheduler, criterion, trainloader, testloader, test_data, prod_num, use_cuda)\u001b[0m\n\u001b[1;32m    108\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# валидируемся\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m hit, mrr \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# test_loss = evaluate_net(net, testloader, use_cuda=use_cuda)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# torch.save(net.state_dict(), 'net.model')\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# логируем после каждой эпохи        \u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrain_loss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)))\n",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_data)\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(opt\u001b[38;5;241m.\u001b[39mbatchSize, \u001b[38;5;28mlen\u001b[39m(test_data[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_data[\u001b[38;5;241m0\u001b[39m]), batch_size)):\n\u001b[0;32m----> 7\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     sub_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m20\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m     targets \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m1\u001b[39m][i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n",
      "Cell \u001b[0;32mIn[39], line 83\u001b[0m, in \u001b[0;36mNet.predict\u001b[0;34m(self, baskets)\u001b[0m\n\u001b[1;32m     81\u001b[0m     padding \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprod_num]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_basket_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(basket_) \u001b[38;5;241m+\u001b[39m global_p \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend(padding\u001b[38;5;241m+\u001b[39mbasket_)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 86\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, prods)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, prods):\n\u001b[0;32m---> 86\u001b[0m     embedd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(embedd)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 33\u001b[0m, in \u001b[0;36mEmbedding_prod.forward\u001b[0;34m(self, prods)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, prods):\n\u001b[0;32m---> 33\u001b[0m     prod_embedd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod_embedd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prod_embedd\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# создаем даталоадер для обучения\n",
    "dataloader_train = DataLoaderStoch(np.array(list(d_basket_train.keys()))[:10],\n",
    "                      d_basket_train,\n",
    "                      item_num=item_num,\n",
    "                      max_basket_size=max_basket_size,\n",
    "                      batchsize=10,\n",
    "                      shuffle=True)\n",
    "\n",
    "# создаем даталоадер для валидации\n",
    "dataloader_val = DataLoaderStoch(np.array(list(d_basket_val.keys())),\n",
    "                      d_basket_val,\n",
    "                      item_num=item_num,\n",
    "                      max_basket_size=max_basket_size,\n",
    "                      batchsize=1024,\n",
    "                      shuffle=False)\n",
    "\n",
    "# учим сеть\n",
    "net_model = train_net(50,\n",
    "                   net_model,\n",
    "                   optimizer,\n",
    "                   scheduler,\n",
    "                   criterion,\n",
    "                   dataloader_train,\n",
    "                   dataloader_val,\n",
    "                   test_data,\n",
    "                   item_num,\n",
    "                   use_cuda=False,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8eebb-dfa8-4e5c-816d-8c54d4b36455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ac473-43e9-42fb-b463-47f344ef535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = next(iter(dataloader_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a19136-6b6c-4b2c-89dd-0ee41e7798ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_model(gen[0]).argmax(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf12d62-7e73-4fae-b9b7-f5fa66d283c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_basket_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8932d-120e-4e49-a684-6a20b84cb30f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next(iter(dataloader_val))[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882901bd-cb01-409c-8675-edb7da953c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
